---
lang: en
title: "Evaluation of Diagnostic Concordance Between Algorithms for Parkinson’s Disease Dementia"
shorttitle: "Parkinson's Disease Dementia Level I Criteria"
author:
  - name: Martina Mana
    corresponding: false
    orcid: "0009-0007-4665-3946"
    role:
      - Conceptualization
      - Data curation
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Josef Mana
    corresponding: false
    orcid: "0000-0002-7817-3978" 
    email: "josef.mana@lf1.cuni.cz"
    role:
      - Conceptualization
      - Data curation
      - Investigation
      - Formal analysis
      - Software
      - Methodology
      - Project administration
      - Validation
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Tereza Uhrova
    corresponding: false
    email: "tereza.uhrova@vfn.cz"
    role:
      - Investigation
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Robert Jech
    corresponding: false
    orcid: "0000-0002-9732-8947"
    email: "jech@cesnet.cz"
    role:
      - Funding acquisition
      - Resources
      - Writing - review & editing
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
  - name: Ondrej Bezdicek
    corresponding: true
    orcid: "0000-0002-5108-0181"
    email: "ondrej.bezdicek@gmail.com"
    role:
      - Investigation
      - Data curation
      - Funding acquisition
      - Conceptualization
      - Project administration
      - Supervision
      - Writing - original draft
    affiliations:
      - id: "1FM"
        name: "First Faculty of Medicine and General University Hospital in Prague, Charles University, Czech Republic"
        department: "Department of Neurology and Centre of Clinical Neuroscience"
abstract: "Background: A significant number of patients with Parkinson’s disease (PD) gradually progress to Parkinson’s disease dementia (PDD). A recent “Call for Change” (Kulisevsky et al., 2024) proposes updates to the current diagnostic criteria for PDD, including the use of different screening tests and broader functional assessments. Objective: We aimed to evaluate the diagnostic concordance between several algorithms for PDD based on Level I (i.e., screening) criteria and to assess their predictive validity for Level II (i.e., neuropsychological battery) diagnosis. Methods: We conducted a cross-sectional retrospective analysis of 204 patients diagnosed with PD. All patients underwent a comprehensive neuropsychological battery. A total of 68 diagnostic algorithms were operationalized based on the combinations of various cognitive assessments, including the Mini-Mental State Examination (MMSE), the Montreal Cognitive Assessment (MoCA), and the shortened version of MoCA (sMoCA). Functional impairment was based on the Functional Assessment Questionnaire (FAQ), specifically FAQ item 9 or the FAQ total score. Concordance between algorithms was assessed using pairwise Cohen’s κ. The predictive validity of Level I algorithms for Level II diagnosis was assessed using Exact Binomial Tests. Results: PDD rate estimates varied widely across algorithms from 2.00% up to 16.75%. Higher rates were associated with functional impairment based on the definition of FAQ score ( = 0.75, = 0.86). Moderate to high concordance was observed among algorithms using the same functional impairment definition. MoCA-based algorithms most accurately predicted Level II classification, especially those using Clock Drawing to assess executive function. Conclusions: Diagnostic outcomes for PDD are sensitive to the choice of cognitive and functional instruments. Our results support some of the proposed changes to PDD diagnostic criteria on Level I,  emphasizing the consistency of MoCA-based assessments and comprehensive functional impairment evaluation."
keywords:
  - Parkinson’s disease
  - dementia
  - diagnostic criteria
  - functional assessment
  - cognitive assessment
format:
 #apaquarto-pdf:
  #  documentmode: man
  #  keep-tex: true
 apaquarto-docx: 
    toc: false
author-note:
  disclosures:
    financial-support: "Supported by the project National Institute for Neurological Research (Programme EXCELES, ID Project No. LX22NPO5107) - Funded by the European Union – Next Generation EU; Charles University: Cooperatio Program in Neuroscience; General University Hospital in Prague project MH CZ-DRO-VFN64165."
bibliography: bibliography.bib
floatsintext: false
numbered-lines: false
suppress-title-page: false
warning: false
echo: false
---

```{r}
#| label: "import"

library(targets)
library(tidyverse)
library(gt)

upstore <- here::here(tar_config_get("store"))
tar_source(here::here("R"))

data <- tar_read(raw_data, store = upstore)
vars <- tar_read(variables, store = upstore)
crit <- tar_read(pdd_data, store = upstore)$algorithms
rates <- tar_read(rate_summaries, store = upstore)

pres <- tar_read(pdd_data, store = upstore)$PDD |>
  mutate(present = if_else(is.na(PDD), 0, 1)) |>
  select(id, type, present) |>
  pivot_wider(names_from = type, values_from = present) |>
  column_to_rownames("id")
```

# Introduction

Parkinson’s disease (PD) is a neurodegenerative disorder typically characterized by a progressive onset of motor symptoms, including rigidity, bradykinesia, postural instability and resting tremor. Moreover, patients suffer from a range of non-motor impairments [@postuma2015], particularly cognitive decline. This factor might result in Parkinson’s disease dementia (PDD) in a subset of patients [@meireles2012].

According to a recent meta-analysis, approximately one-quarter of PD patients are likely to be diagnosed with PDD [@sousa2022]. However, reported PDD rate estimates vary widely, ranging from 14% up to 55%, depending on the methodological criteria employed. Moreover, factors such as patients' sex [@cereda2016], age and disease duration appear to modulate the risk of cognitive decline and PDD [@oh2016; @rana2012].

Despite the clinical relevance of PDD, its diagnosis remains complex. A milestone in research of PDD was the publication of diagnostic criteria established in 2007 by the International Parkinson and Movement Disorder Society (MDS) [@dubois2007]. In these criteria, the MDS introduced a two-levelled system for PDD detection. Level I consists of brief cognitive assessments, while Level II involves comprehensive neuropsychological testing across cognitive domains [@emre2007].

The original Level I algorithm included eight conditions that had to be satisfied simultaneously in order to diagnose probable PDD. These included: 1) diagnosis of PD proposed by the Queen Square Brain Bank; 2) PD onset prior to the PDD emergence; 3) evidence of global cognitive impairment (MMSE score \< 26 points); 4) cognitive deficit interference with the IADL (assessed by the pill questionnaire or caregiver interview); 5) impairment in at least two cognitive domains, namely memory, attention, visuo-constructive abilities and executive function; 6) there was absence of Major Depressive Disorder; 7) absence of delirium; and 8) exclusion of other abnormalities and potential causes of dementia [@dubois2007].

Currently, efforts are focused on refining this PDD diagnostic framework. A recent call for a change pinpoints limitations regarding the original criteria and suggests various updates to enhance their utility [@kulisevsky2024]. Proposed suggestions include replacement of the Mini-Mental State Examination (MMSE) by the Montreal Cognitive Assessment (MoCA), which is more sensitive to PD-specific cognitive impairment; expansion of instrumental activities of daily living (IADL) evaluation; inclusion of language assessment; recognition of anxiety as one **of** the neuropsychiatric symptoms relevant in PDD; and integration of biomarkers.

In light of these proposals, the current study aims to evaluate the diagnostic concordance between the original MDS Level I PDD criteria [@emre2007; @dubois2007] and a modified framework based on the recent call for change [@kulisevsky2024]. Furthermore, both Level I diagnostic approaches are compared to PDD diagnosed on Level II. The study aims to address the following research objectives (RO): (RO1) To estimate the PDD rate and evaluate the diagnostic variability and concordance across different PDD criteria. (RO2) To identify specific diagnostic components contributing to PDD classification variability across the applied criteria.

# Methods

## Participants

```{r}
#| label: "dates"

continue <- all(!is.na(data$assdate))
stopifnot("Some assessment dates are missing" = continue)

dates <- sapply(c("min", "max"), function(f) {
  date <- do.call(f, list(data$assdate))
  paste(month(date, label = TRUE, abbr = FALSE), year(date))
})
```

This study retrospectively analyzed clinical data from a cohort of patients with PD at the General University Hospital in Prague. All patients were diagnosed with idiopathic PD by a movement disorder specialist according to the MDS Clinical Diagnostic Criteria for PD [@postuma2015]. Clinical records spanning `r dates["min"]` to `r dates["max"]` were examined. All participants were candidates for Deep Brain Stimulation (DBS) treatment and underwent neuropsychological evaluation conducted by a trained clinical psychologist (OB) as part of standard preoperative assessments for DBS eligibility at the General University Hospital in Prague.

## Neuropsychological Assessment

Cognitive performance was evaluated at both Level I and Level II according to the standard MDS battery for Parkinson’s Disease Mild Cognitive Impairment (PD-MCI) [@litvan2012, @bezdicek2017]. Cognitive performance at Level I was assessed by the MMSE [@stepankova2014; @folstein1975] and the MoCA [@kopecek2016; @nasreddine2005]. The comprehensive neuropsychological assessment at Level II evaluated five cognitive domains through specific tests: attention and working memory assessed by Trail Making Test Part A (TMT-A) [@bezdicek2012; @reitan2004], and WAIS Digit Span Backward (WAIS DSB) [@wechsler1997], executive function by Categorical Verbal Fluency - Animals (CF-A) [@nikolai2015], and subtest from the Prague Stroop Test – Colors (PST-C) [@bezdicek2021], language by the WAIS Similarities subtest [@wechsler1997], and the Boston Naming Test (BNT-60) [@zemanova2016; @kaplan1983], memory by the Rey Auditory Verbal Learning Test (RAVLT-DR) [@frydrychova2018; @bezdicek2013; @rey1964] delayed recall, and the Brief Visual Memory Test–Revised (BVMTR-DR) [@havlik2020; @benedict1997] delayed recall, or WAIS Family Pictures subtest [@wechsler1997] delayed recall, visuospatial function assessed by the Judgment of Line Orientation Test (JoL) [@benton1983], and Clock Drawing Test (CLOX-I) [@royall1998].

The Functional Activities Questionnaire (FAQ) [@bezdicek2016; @pfeffer1982] was administered to assess functional impairment. The Beck Depression Inventory-II (BDI-II) [@ciharova2020; @beck1996] and State-Trait Anxiety Inventory (STAI) [@spielberg1983; @mullner1980] were used to assess neuropsychiatric status.

## Diagnostic algorithms for probable Parkinson's Disease Dementia

In this study, we applied three distinct sets of diagnostic algorithms for probable PDD at Level I. The first set was based on the original framework [@dubois2007], which utilized the MMSE as a global cognitive screening tool, supplemented by assessments of attention, executive function, visuospatial abilities, and memory. The second set of algorithms was based on the recent call for change of dementia diagnostic guidelines [@kulisevsky2024], which advocates for more sensitive cognitive domain assessments in the context of PD. This updated approach incorporated specific items from the MoCA. The third approach applied the Czech version of the shortened Montreal Cognitive Assessment (sMoCA) [@bezdicek2020], a time-efficient modification designed to measure global cognitive performance using a reduced testing protocol that omits items providing redundant information. **The sMoCA has been validated in the Czech PD cohort [@bezdicek2020] and shown to be sensitive to cognitive deficits while lowering patient burden [@roalf2017a]. We included the sMoCA in our study for its clinical utility in pre-surgical settings, where time restrictions and patients’ fatigue often limit the feasibility of longer assessments. Moreover, the Czech validation study reported comparable diagnostic accuracy between MoCA (AUC = 0.815) and sMoCA (AUC = 0.796) for distinguishing PD-MCI from PD-NC, supporting the sMoCA as a suitable and efficient alternative.**

Lastly, the fourth approach followed the Level II protocol for diagnosis of PDD and Mild Cognitive Impairment in PD (PD-MCI) [@dubois2007; @litvan2012]. The Level II methodology, including the use of a regression-based normative scoring approach, has been detailed in a prior study [@bezdicek2017]. In this study, the thresholds for cognitive impairment at Level II were set at $z \leq -1.5$. All non-cognitive criteria of probable PDD (i.e., diagnosis of PD that developed before dementia and absence of Major Depression, delirium or other abnormalities that obscure diagnosis) held true for all patients in the sample according to the psychiatric and neurological examinations.

```{r}
#| label: "algo-check"

algonum <- sapply(c("mmse", "moca", "smoca", "lvlII"), function(i) {
  subset(crit, group == i) |>
    nrow()
})

continue <- !any(algonum == 0)
stopifnot("Some algorithms are missing" = continue)
```

For each of these diagnostic approaches, we applied two operationalizations of deficits in Instrumental Activities of Daily Living (IADL). First, we utilized FAQ item 9, which approximates the pill questionnaire from the original criteria [@dubois2007] employing a cut-off score of 2 points or higher. Second, we applied the entire Functional Activities Questionnaire (FAQ) as suggested in the call for change [@kulisevsky2024], employing a cut-off score of 7 points or higher based on Czech normative data [@bezdicek2011]. These methodologies resulted in a total of `r sum(algonum)` algorithms, which were distributed across different diagnostic criteria: `r algonum["mmse"]` MMSE-based, `r algonum["moca"]` MoCA-based, `r algonum["smoca"]` sMoCA-based, and `r algonum["lvlII"]` based on the Level II battery (see @fig-tree, @tbl-crits and Appendix @tbl-algos for the exact specification of each algorithm). 

**Finally, all patients were systematically evaluated for the presence of neuropsychiatric symptoms, including depression, apathy, anxiety, psychosis, and delirium, by a trained neuropsychiatrist (TU) experienced in the assessment of patients with movement disorders. Because severe psychiatric symptoms form exclusion criteria for the diagnosis of probable PDD (conditions 6–8; p. 1), all patients classified as PDD were double-checked in hospital records to confirm the absence of such confounding symptoms.**

```{r}
#| label: "tbl-crits"
#| tbl-cap: "Summary of probable PDD operationalizations compared in the study."

table_algorithms(vars)
```

\[Insert Table 1 here\]

## Statistical Analyses

Following the framework proposed by @lundberg2021, in this study we explicitly connect our research objectives and their corresponding theoretical (i.e., targets of inference) and empirical (i.e., data-driven) estimands to statistical estimates. The theoretical estimand refers to a unit-specific quantity defined over a target population and represents the ideal quantity that would address the research question under optimal conditions, such as access to complete population data or perfect experimental control. In contrast, the empirical estimand corresponds to the quantity that is actually computable using the available dataset, given real-world constraints. A full description of the study’s estimands and their relation to our research objectives is presented in the Appendix (see @tbl-estimands).

To address study objectives, we started by repeatedly assigning each patient the diagnosis of probable PDD based on each PDD algorithm listed in @tbl-crits (see also @tbl-algos) resulting in a `r sum(data$incl)` (patients) $\times$ `r sum(algonum)` (algorithms) matrix where each cell indicates whether a patient (row) meets criteria for probable PDD according to an algorithm (column). PDD rate estimates were computed as $\frac{N_{PDD}}{N_{total}}$ separately for each algorithm. The predictive value of age and sex was then evaluated by fitting a set of logistic regressions, one for each algorithm for probable PDD, whereby the probable PDD was predicted by age, sex and their interaction.

Next, a set of two class cross-tabulations with associated statistics was computed for each pair of algorithms via the `confusionMatrix()` function from the R package *caret* [@kuhn2008]. For each pair of algorithms, the analysis was repeated twice such that each variable of the pair served once as the reference and once as the predictor. The following measures were used to evaluate pairwise concordance between different algorithms for probable PDD: 1) Cohen's $\kappa$ with its 95% confidence interval (CI) computed via the `cohen.kappa()` function from the R package *psych* [@revelle2024]; 2) Accuracy (i.e., the proportion of correct predictions, both true positives and true negatives, among the total number of cases) with its 95% CI; 3) Sensitivity/Recall (i.e., the proportion of true positives); and 4) Specificity (i.e., the proportion of true negatives).

Finally, the No Information Rate (NIR) was calculated for each pair of algorithms. NIR is the accuracy that could be obtained by always predicting the majority class and in our case it is equivalent to the complement of the PDD rate estimate according to the reference algorithm. The accuracy of prediction was compared to the NIR via a one-sided Exact Binomial Test as implemented by the `binom.test()` R stats function. Reference/predictor pairs associated with p \< .05 were considered to show significantly better accuracy than NIR. In other words, for reference/predictor pairs associated with p \< .05, we conclude that knowing the probable PDD status according to the predictor algorithm helps to estimate the probable PDD status according to the reference algorithm and the two algorithms thus show substantial concordance.

**Missing data were handled by complete cases analysis and pairwise complete cases analysis. In other words, each univariate analysis and pairwise comparison used all available data.** Data wrangling and visualizations were done in the *tidyverse* package [@wickham2019] and tables were formatted in the *gt* package [@iannone2024]. All analyses were conducted within the R (version `r with(version, paste(major, minor, sep="."))`) software environment for statistical computing [@rsoft]. The software code supporting this article is available at <https://github.com/josefmana/demcrit.git>.

# Results

```{r}
#| label: "results"

desc  <- tar_read(sample_description, store = upstore)
algos <- tar_read(algorithms, store = upstore)
pred  <- tar_read(demographic_predictors, store = upstore)
conc  <- tar_read(concordance_statistics, store = upstore)
kappa <- tar_read(kappa_summmaries, store = upstore)
perc  <- rates$table$perc

# Tables of the 'best' and the 'worst' w.r.t. Level II:
approx_acc <- table_levelII_approximations(conc$table, algos, "Accuracy_raw")
approx_bal <- table_levelII_approximations(conc$table, algos, "Balanced Accuracy")

# Total numbers:
n_total <- sum(data$incl)
id_excl <- names(which(rowSums(pres) == 0))
n_excl <- length(id_excl)
n_incl <- n_total - n_excl
if (n_excl == 1) {
  excl_txt <- paste0("one patient was excluded due to missing neuropsychological data, resulting in a final sample of ", n_incl, " patients")
} else if (n_excl > 1) {
  excl_txt <- paste0(english::english(n_excl), " patients were excluded due to missing neuropsychological data, resulting in a final sample of ", n_incl, " patients")
} else {
  excl_txt <- "no patients was excluded."
}

# Function for writing M (SD):
msd_txt <- function(x, txt = "", tab = desc$table) {
  paste0(
    subset(tab, variable == x)[ , c("M", "SD")] |> paste(collapse = " (SD = "),
    ")",
    txt
  )
}

# In-text demographic data
sex <- subset(desc$table, variable == "Sex")$N
age <- msd_txt("Age", " years of age")
edu <- msd_txt("Education", " years of education")
dur <- msd_txt("PD duration", " years of disease duration")
updrs_off <- msd_txt("UPDRS III off state", " Unified Parkinson Disease Rating Scale (UPDRS), part III in medication OFF state")
updrs_on <- msd_txt("UPDRS III on state", " Unified Parkinson Disease Rating Scale (UPDRS), part III in medication ON state")
bdi <- msd_txt("BDI")
staix1 <- msd_txt("STAI X1")
staix2 <- msd_txt("STAI X2")

# In-text rate estimates
rates_overall <- sapply(c("M", "SD", "Md", "minmax"), \(f) do_summary(perc, 2, f))
rates_FAQ9 <- sapply(c("M", "SD", "Md", "minmax"), \(f) do_summary(perc[grepl("FAQ 9", rates$table$IADL)], 2, f))
rates_FAQtot <- sapply(c("M", "SD", "Md", "minmax"), \(f) do_summary(perc[rates$table$IADL == "FAQ > 7"], 2, f))

# Minimum p-value of logistic regressions:
min_p <- subset(pred$values, quantity == "p-value")$estimate |>
  min() |>
  do_summary(3, "p")
```

## Sample Description

**A total of** `r n_total` patients **were considered for the study, out of which `r excl_txt`**. The sample included `r sex` men, with an average of `r age`, `r edu`, `r dur`, `r updrs_off` and `r updrs_on`. **Descriptive statistics for neuropsychiatric symptoms indicated within average levels of depressive and anxiety symptoms in our cohort, with the average BDI-II of `r bdi`, average STAI X1 of `r staix1`, and average STAI X2 of `r staix2`. However, according to the psychiatric assessment, none of the patients with probable PDD was suffering from the major depressive disorder, delirium or other neuropsychiatric abnormalities that would exclude the diagnosis.** Cognitive characteristics of the sample are summarized in @tbl-desc.

```{r}
#| label: "tbl-desc"
#| tbl-cap: "Cognitive characteristics of the sample."

desc$gtable
```

\[Insert Table 2 here\]

## PDD Rate Estimates

```{r}
#| label: "observations-univariate"

obs <- rates$table|>
  count(N) |>
  mutate(perc_missing = glue::glue("{round(100 * (n_incl-N) / n_incl, 1)}%"))
```

Algorithm-wise rate of PDD estimates **corresponding numbers of patients with available data** are presented in @tbl-rates. **In total, there were `r obs[1, "n"]` algorithms based on `r obs[1, "N"]` complete observations (`r obs[1, "perc_missing"]` missing). One algorithm used `r obs[2, "N"]`, `r obs[3, "N"]`, and `r obs[4, "N"]` complete observations, respectively (`r obs[2, "perc_missing"]`, `r obs[3, "perc_missing"]` and `r obs[4, "perc_missing"]` missing). A further `r obs[5, "n"]` algorithms were based on `r obs[5, "N"]` complete observations (`r obs[5, "perc_missing"]` missing), and `r obs[6, "n"]` algorithms included the full sample of `r obs[6, "N"]` patients.**

On average, the estimated PDD rate was `r rates_overall["M"]`% (SD = `r rates_overall["SD"]`, Md = `r rates_overall["Md"]`, range `r rates_overall["minmax"]`). Notably, the estimates were substantially lower when FAQ item 9 was used as a criterion of IADL deficit (M = `r rates_FAQ9["M"]`% SD = `r rates_FAQ9["SD"]`, Md = `r rates_FAQ9["Md"]`, range `r rates_FAQ9["minmax"]`) compared to using the total FAQ score criterion (M = `r rates_FAQtot["M"]`% SD = `r rates_FAQtot["SD"]`, Md = `r rates_FAQtot["Md"]`, range `r rates_FAQtot["minmax"]`) as demonstrated in @fig-tree **(see also @fig-rates for per-operationalization distribution of PDD rate estimates)**. Neither age, sex nor their interaction (*p*s $\geq$ `r min_p`) reliably predicted probable PDD classification across algorithms (see @fig-data and @fig-pars).

\[Insert Figure 1 here\]

```{r}
#| label: "fig-tree"
#| fig-cap: "A dendrogram representing algorithms for probable Parkinson's Disease Dementia (PDD) construction process."
#| apa-note: "The dendrogram illustrates the decision process used to construct algorithms for probable Parkinson’s Disease Dementia (PDD). The second level depicts the definition of instrumental activities of daily living (IADL) deficit (FAQ total > 7 on the left, FAQ item 9 > 1 on the right). The third level indicates the selection of the screening instrument (MMSE, MoCA, sMoCA, or none in the case of Level II). Lower branches represent the selection of neuropsychological tests used to define cognitive impairment in executive function, attention, memory, and language, ordered from top to bottom as depicted in the dendrogram. Algorithms based on the MMSE are shown in green, those based on the MoCA in blue, on the sMoCA in purple, and Level II algorithms in red. The top five screening algorithms according to raw accuracy for predicting diagnosis at Level II (see Table 3) are marked by bold edges. Accompanying dots indicate the estimated PDD rate for each algorithm. The individual test items comprising each algorithm are listed in Table A1."

make_dendrogram(rates$table, approx_acc$table)
```

## Concordance between Algorithms

```{r}
#| label: "observations-bivariate"

# Dimensions of the results table:
dims <- glue::glue("{dim(conc$table)[1]} rows x {dim(conc$table)[2]} columns")

# Number of observations per algorithm pair:
obs_pairs <- conc$table |>
  count(N) |>
  drop_na() |> # NA for cases where reference == predictor
  mutate(perc = 100 * n / sum(n))

# The most missing values present:
min_miss <- glue::glue("{round(100 * (n_incl - obs_pairs[1, 'N']) / n_incl, 2)}%")

# Percentage of algorithm pairs with N > K-1
K <- 200
perc_above_k <- obs_pairs |>
  filter(N >= K) |>
  pull(perc) |>
  sum() |>
  do_summary(2)
```

Results of the analyses of prediction Accuracy, Cohen's $\kappa$, Sensitivity and Specificity are presented in @fig-acc, @fig-kappa, @fig-sens and @fig-spec respectively. **The number of complete pairwise cases ranged from `r obs_pairs[1, "N"]` (`r min_miss` missing) to `r obs_pairs[nrow(obs_pairs), "N"]` (full sample). Most comparisons (`r perc_above_k`%) were based on `r K` or more observations.**[^1]

[^1]: **Due to the large number of entries (`r dims` representing pairwise comparisons and metrics of interest respectively), the table with numerical results is not presented here or in the Appendix. Instead, we share the table share as data in the accompanying R package available at <https://github.com/josefmana/demcrit.git>. To obtain the table in format not dependent on R, follow the tutorial at <https://josefmana.github.io/demcrit/articles/concordance.html>**

Generally, algorithms that employed the same operationalization of IADL deficit showed substantial pairwise concordance, however, algorithms that operationalized IADL deficit differently did not. Whereas among algorithms with identical IADL deficit operationalization, the agreement judged by Cohen's $\kappa$ was moderately high (operationalization by FAQ total score: $\kappa$ = `r kappa$sums["faq_tot"]`; operationalization by FAQ item 9: $\kappa$ = `r kappa$sums["faq_9"]`), among algorithms that differ in IADL deficit operationalization but are otherwise identical it was low: $\kappa$ = `r kappa$sums["iadl"]`.

\[Insert Figure 3 here\]

```{r}
#| label: "fig-acc"
#| fig-cap: "Prediction accuracy matrix."
#| apa-note: "The matrix depicts classification accuracy of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response. Cases with asterisk indicate predictive accuracy statistically significantly higher than the No Information Rate."

conc$plots$Accuracy
```

## Prediction of Level II Criteria

```{r}
#| label: "lvlII-stats"

lvlII_rates <- c(
  faq_tot = subset(rates$table, type == "Lvl.II (1)")$perc |> do_summary(2) |> paste0("%"),
  faq_9 = subset(rates$table, type == "Lvl.II (2)")$perc |> do_summary(2) |> paste0("%")
)

smoca_stats <- c(
  spec = subset(conc$table, predictor == "sMoCA (1)" & reference == "Lvl.II (1)")$Specificity |> do_summary(2),
  sens = subset(conc$table, predictor == "sMoCA (1)" & reference == "Lvl.II (1)")$Sensitivity |> do_summary(2)
)
```

For easier interpretability of our results, we next examined cases where Level II algorithms served as a reference and Level I algorithms as a predictor. @tbl-approx shows five Level I algorithms with the highest and five with the lowest accuracy in predicting Level II classification of probable PDD.

When the IADL deficit was defined by total FAQ score, the Level II estimate of PDD rate was `r lvlII_rates["faq_tot"]`. All five Level I algorithms that approximated the Level II classification most accurately were MoCA-based and defined Executive Function deficit by Clock drawing rather than the Verbal fluency test. On the other hand, two out of the five Level I algorithms with the lowest accuracy were MMSE-based, whereas the remaining three were MoCA-based and defined Executive function deficit by the Verbal fluency test.

When the IADL deficit was defined by the FAQ item 9 score, the Level II estimate of PDD rate was `r lvlII_rates["faq_9"]`. Overall, the difference between the most accurate and the least accurate Level I algorithms was lower than in the case of IADL deficit being defined by FAQ total score (see @tbl-approx). The five most accurate algorithms were all MoCA-based, defined Executive Function deficit by Clock drawing (with threshold \< 2) and in the majority of cases defined Language deficit by Animal naming. Two out of the five Level I algorithms with the lowest accuracy were MMSE-based, whereas the remaining three were MoCA-based and defined Executive Function deficit by Clock drawing (with threshold \< 3) and Language deficit by Abstraction.

Finally, suppose the predictors are sorted by their balanced accuracy (i.e., average of sensitivity and specificity) instead of raw accuracy. In that case, the results are similar, with the exception that for the prediction of Level II with total FAQ score algorithm for probable PDD, the highest balanced accuracy was achieved by the sMoCA algorithm with sensitivity `r smoca_stats["sens"]` and specificity `r smoca_stats["spec"]` (see @tbl-approx-bal).

```{r}
#| label: "tbl-approx"
#| tbl-cap: "Level I algorithms for probable PDD as predictors of Level II classification as the reference."

approx_acc$gtable
```

# Discussion

This study systematically investigated the application of multiple Level I PDD diagnostic criteria. Our results show variability in PDD rate estimates, strongly influenced by the choice of cognitive screening instrument (MMSE, MoCA and sMoCA) and the operationalization of functional impairment. The divergence observed across algorithms demonstrates the sensitivity of diagnostic outcomes to seemingly negligible methodological choices.

## Variability in PDD Rate Estimates

```{r}
#| label: "rates-minmax"

rate_min <- sub("-.*", "", rates_overall["minmax"])
rate_max <- sub(".*-", "", rates_overall["minmax"])
```

Our results showed a wide range in estimated PDD rate across algorithms, ranging from `r rate_min`% to `r rate_max`%. Estimates reached lower rates when using solely FAQ item 9 (as an approximation of the pill questionnaire suggested by @dubois2007) in comparison with the full FAQ scale. This discrepancy highlights the diagnostic importance of how IADLs are assessed.

Our overall PDD rates were consistently lower than previous studies regarding **dementia** among PD patients, demonstrating wide variability based on various criteria used. For instance, a retrospective study reported a PDD rate of 19.7% [@rana2012], while other clinical investigations found even higher rate, reaching up to 30% [@aarsland2005]. A recent meta-analysis synthesizing global data placed the expected PDD rate in PD at 26.30% [@sousa2022]. Compared to these estimates, our study reports generally lower PDD rates, likely reflecting differences in diagnostic criteria, methodology and sample characteristics. Specifically, our sample was younger compared to other PD cohorts and age was repeatedly shown to be a strong predictor of PDD across studies [@rana2012; @sousa2022].

```{r}
#| label: "age-over-70"

over70 <- data |>
  mutate(older = 100 * (age >= 70)) |>
  pull(older) |>
  mean(na.rm = TRUE) |>
  round(1) |>
  paste0("%")
```

Interestingly, we did not observe any reliable age-related differences in PDD rate within our cohort. This lack of age-dependency may, however, also stem from the relatively younger age of our cohort, because previous reports indicate that the association between age and PDD is not linear but increases with age and may not reach substantial values before **older age. In both @rana2012 and @oh2016, nine out of ten patients with dementia were 70 years of age or older. In our sample, only `r over70` participants were in this age range. Consequently, studies with older cohorts are probably necessary to detect a robust association between age and the risk of probable PDD.**

## Concordance Between Diagnostic Algorithms

Pairwise comparisons of diagnostic algorithms showed that agreement was notably stronger among those using the same IADL operationalization compared to those using different IADL definitions. Moreover, the agreement was slightly higher between algorithms that defined IADL deficit by FAQ item 9 compared to algorithms that defined it using the full FAQ scale. One possible explanation of this difference follows from the observation that algorithms using the full-scale definition yielded higher PDD rate estimates. Because there was a higher probability of being diagnosed with IADL deficit based on the full FAQ scale, there was also a bigger room for disagreement in the cognitive impairment status when different indexes were used (e.g. by defining executive deficit via Clock Drawing vs. Verbal Fluency).

Overall, when the same IADL definitions were used across algorithms, we observed concordance levels varying from moderate (using FAQ total score) to strong (using FAQ item 9), consistent with inter-rater reliability analysis [@mchugh2012]. Contrarily, the concordance between algorithms using different IADL deficit definitions was equivalent to minimal agreement. This demonstrates that even slight methodological differences can yield divergent diagnostic outcomes. Such findings are critical for clinicians relying on Level I criteria for eligibility decisions, as the choice of algorithm could lead to contradictory classifications of PDD status.

## Predictive Validity Comparison With Level II Criteria

Using Level II diagnosis as the gold standard, MoCA-based Level I algorithms, particularly those using Clock Drawing to assess executive function, demonstrated the highest predictive accuracy. This supports recent proposals to modernize PDD diagnostic frameworks [@kulisevsky2024], favoring MoCA-derived components and more ADL-specific, PD-tailored functional assessment tools. In contrast, MMSE-based algorithms consistently underperformed, suggesting limited sensitivity in capturing cognitive deficits typical in PDD.

Furthermore, in the algorithm using sMoCA, the raw accuracy was moderate, however, the balanced accuracy (i.e. combined sensitivity and specificity) was high. **Consequently, sMoCA appears particularly suitable for approximating Level II PDD diagnosis in populations that differ in PDD prevalence from our cohort, since balanced accuracy, unlike raw accuracy, is independent of prevalence in the sample. Moreover, because the sMoCA algorithm demonstrated higher sensitivity while maintaining comparable specificity (see @tbl-approx-bal), it may be especially valuable in contexts where false negatives carry substantial clinical cost. In such cases, a neuropsychologist might use sMoCA as an initial screening tool and proceed to a full Level II assessment only for patients who meet criteria for probable PDD in this preliminary stage.**

## Constraints on Generality

**This study’s generalizability is limited by the homogeneity of the patient cohort, which does not reflect the diversity of cognitive profiles seen in broader PD populations. Specifically, the younger age of the sample, and a possible underrepresentation of high-risk phenotypes for PDD constrain the generality of the presented findings.**

**As noted above, the younger age of our sample may partly explain the lower rate of PDD observed compared to previous studies. As discussed in the *Theoretical and Empirical Estimands* section of the Appendix, neither estimates of PDD rate nor predictive performance of demographic variables therefore should not be generalized beyond PD patients who are DBS candidates. The extent to which our findings on the concordance between diagnostic algorithms apply to the broader PD population remains to be determined in future studies using different types of cohorts, such as de novo patients or community-based samples.**

**Longitudinal cohort studies suggest that patients considered for DBS represent a subset of the PD population with a distinct cognitive phenotype [@bove2020; @mana2024]. Specifically, findings of gradual post-surgical cognitive decline predicted by pre-surgical executive deficits indicate that DBS candidates may be preferentially drawn from a fronto-striatal phenotype characterized by slowly progressing executive dysfunction, rather than from a posterior phenotype marked by visuospatial impairment [@kehagia2012]. Importantly, patients with the posterior phenotype may be at greater risk of developing PDD within as little as five years after disease onset [@williams-gray2009; @summers2024].**

**In our study, visuospatial function was assessed uniformly across all algorithms within a given screening measure (MoCA cube or MMSE pentagons). By contrast, we compared two operationalizations of executive dysfunction, the clock drawing test and verbal fluency. The clock drawing test showed stronger predictive value for level II diagnosis than verbal fluency, suggesting that even within our cohort, patients who developed PDD may exhibit features of the posterior phenotype.**

**However, the use of a DBS cohort also offered several methodological advantages. All patients underwent standardized and comprehensive neuropsychological testing, resulting in a well-characterized dataset that enabled a systematic evaluation of multiple diagnostic algorithms. Moreover, because dementia is a common exclusion criterion for DBS treatment [@bronstein2011], examining the diagnostic accuracy of algorithms for PDD in a pre-DBS cohort is informative in its own right.**

## Limitations and Future Directions

**Due to the retrospective nature of the study, some patients lacked one or more key measures required for the diagnosis of probable PDD by certain algorithms. Missing data were handled using the pairwise complete cases method. The main advantages of this approach are its straightforward implementation and preservation of statistical power. However, it may introduce bias, particularly when data are not missing completely at random (MCAR) and when causal inference is the goal [@little2019].**

**Modern alternatives, such as multiple imputation techniques, have been shown to produce less biased estimates than case deletion strategies in analyses based on confusion matrices [@karakaya2014], whereas the evidence supporting their superiority in the estimation of Cohen’s $\kappa$ remains limited [@deraadt2019]. These modern approaches also require careful specification of the imputation model and the underlying causal mechanism of missingness to ensure appropriate covariate selection [@long2011; @bianco2023]. For the sake of parsimony, we did not employ advanced missing-data techniques but instead explicitly described the observed missingness patterns. Consequently, our findings should be regarded as exploratory and primarily serve as a basis for hypothesis generation in future, more controlled studies.**

**Another possible avenue for analysing the present data would be to exploit their hierarchical structure by fitting a multilevel model with algorithms nested within cognitive tests (see @fig-tree). Such an approach could leverage partial pooling [@gelman2012] and might approximate modern item response theory frameworks [@burkner2021], where PDD is conceptualized as a latent trait and the individual diagnostic algorithms act as items measuring it. If feasible, this psychometric perspective may represent an exciting new direction for conceptualizing complex clinical phenomena such as PDD (cf. @miller2012, @kiselica2021).**

**An additional limitation concerns the use of the FAQ questionnaire for IADL assessment. The FAQ is a subjective or informant-reliant measure and thus susceptible to bias. Moreover, its content can vary across sociocultural contexts, which limits its cross-cultural transferability. For example, activities such as financial management, cooking or driving, are not universally practiced across societies. Consequently, both the FAQ scores and diagnostic thresholds used to indicate IADL impairment may not be directly transferable between cultural settings [@odonald2025]. These factors may influence both the sensitivity and ecological validity of the functional criteria used [@bezdicek2011; @bezdicek2016b].**

**Furthermore, IADL measures may correlate with neuropsychological results of specific cognitive domains, particularly attention/processing speed, and executive function [@reppermund2011; @moheb2017]. If such correlations were due to shared error variance, they could bias concordance indices measured in this study. To examine this possibility, we conducted a post-hoc simulation experiment, available at <https://josefmana.github.io/demcrit/articles/correlation.html>. The results indicated that correlations between IADL and neuropsychological measures may either increase or decrease accuracy and balanced accuracy, while consistently inflating Cohen’s $\kappa$ estimates. However, given the correlations observed in our dataset, the effect size was small and unlikely to alter the conclusions of our study.**

**To address concerns about measuring IADL outlined above, future research should consider using PD-specific questionnaires or more objective tools. Promising options include the Penn Parkinson's Daily Activities Questionnaire-15 [@brennan2016], questionnaire adaptations including items regarding gadget use and digital literacy [@postema2024] or performance-based assessments [@schmitter-edgecombe2020]. Our findings underscore the importance of IADL measurement for the PDD diagnosis. Therefore, we recommend exploring more reliable tools with high ecological validity.**

Finally, whereas our study systematically investigated how varying definitions of global deficit, impaired cognition and IADL deficit affect probable PDD classification, it did not explore associations of PDD diagnosis with its neuropsychiatric (e.g. anxiety profile) and biomarker correlates. **Instead, we only ensured the absence of acute or severe psychiatric symptoms that would preclude a diagnosis of probable PDD, as verified through assessment by a trained neuropsychiatrist. However, both the current diagnostic criteria for PDD [@dubois2007] and recent proposals for their revision [@kulisevsky2024] emphasize the use of standardized psychometric instruments for assessing neuropsychiatric symptoms. Incorporating such measures could enhance both the efficiency and transparency of the diagnostic process. Future research should therefore investigate how integrating structured neuropsychiatric assessments and biomarker data may refine PDD diagnostic accuracy and improve clinical utility.**


# Conclusions

In sum, our study systematically investigated how varying definitions of impaired cognition and IADL deficit affect PDD diagnostic accuracy. Our study highlights the variability in PDD classification across Level I diagnostic algorithms, significantly influenced by IADL operationalization and the choice of cognitive screening tools. The findings strongly support the call for a change of the current diagnostic criteria [@kulisevsky2024], favoring the use of MoCA-based components and comprehensive IADL assessments.

Conservative criteria, such as reliance on pill questionnaire (i.e. FAQ item 9 equivalence), may fail to detect functional decline and thus under-identify true cases of PDD. Importantly, concordance across algorithms rises significantly, reaching moderate to high values, when the same definition of IADL is used (either FAQ total or FAQ item 9). Moreover, when using MoCA-based algorithms instead of MMSE-based ones, we can observe better approximations to the Level II battery.

Future studies should aim to replicate our results on larger and different cohorts **with more heterogeneous sample, and to explore how varying operationalization of other components of probable PDD, namely psychiatric symptoms, affects PDD rates and concordance between diagnostic algorithms**. To make this process easier, the code used to generate our results is publicly available and easily applicable to similarly structured data.

# References

# Appendix

## Derivation of the Algorithms Set

Both, the original PDD criteria [@dubois2007] and the call for their change [@kulisevsky2024] allow for several distinct combinations of items to be used to define cognitive impairment. Consequently, in this study we derived all algorithms for probable PDD on Level I that are in line with published criteria. This procedure parallel the diagnostic algorithm outlined in Table 2 of @dubois2007. Specifically, in this study, we varied the exact specification of items 3-5 of this table (i.e., the measure of global cognitive impairment, the measure of the impact on IADLs and the measures of impaired cognition).

For each set of criteria (MMSE-based, MoCA-based, sMoCA-based and Level II), we first specified the items and then the thresholds for each item used to define probable PDD. If more than one option was present in either the choice of the item or the choice of the threshold, we created an algorithm for each choice in turn. The final set of algorithms was arrived at by computing the Cartesian product of all possibilities provided by varying items and thresholds. All combinations are presented in @tbl-algos.

For MMSE-based algorithms, the following sets of items served as the basis:

$Global = \{MMSE < 26\}$

$Attention = \{{Sevens\ backwards < 4}\}$

$Executive = \{{Clock\ drawing} < 2, {Lexical\ fluency\ (S)} < 10\}$

$Construction = \{Pentagons < 1\}$

$Memory = \{{Three{\text-}words\ recall} < 3\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

The ensuing Cartesian product $Global \times Attention \times Executive \times Construction \times Memory \times IADL$ results in $1 \times 1 \times 2 \times 1 \times 1 \times 2 = 4$ MMSE-based algorithms for probable PDD.

For MoCA-based algorithms, the following sets of items served as the basis:

$Global = \{MoCA < 26\}$

$Attention = \{{Sevens\ backwards < 3}\}$

$Executive = \{{Clock\ drawing} < 2, {Clock\ drawing} < 3, {Lexical\ fluency\ (K)} < 11\}$

$Construction = \{Cube\ drawing < 1\}$

$Memory = \{Five{\text-}words\ recall < 1, Five{\text-}words\ recall < 2, Five{\text-}words\ recall < 3, Five{\text-}words\ recall < 4, Five{\text-}words\ recall < 5\}$

$Language = \{Abstraction < 2, Animal\ naming < 3\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

Note that the additional language domain adds complexity to establishing a diagnostic algorithm because simply by adding it to the set of items, the number of potential algorithms doubles. Further complexity is added by the fact that there are so far no guidelines for selecting a diagnostic threshold for Clock drawing and Five-words recall tests, both of which differ from their counterparts used by @dubois2007. Finally, although the Sevens backwards item has different thresholds in MoCA-based compared to MMSE-based algorithms, this difference is solely due to a difference in scoring whereby 3 points in MoCA correspond to 4 or 5 points in MMSE. The Seven backwards item threshold for MoCA-based algorithms used in this study is thus equivalent to its MMSE-based counterpart.

Computing the Cartesian product $Global \times Attention \times Executive \times Construction \times Memory \times Language \times IADL$ yields $1 \times 1 \times 3 \times 1 \times 5 \times 2 \times 2 = 60$ distinct MoCA-based algorithms for probable PDD.

For sMoCA-based algorithms, the following sets of items served as the basis:

$Global = \{sMoCA < 13\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

yielding $Global \times IADL$, i.e., $1 \times 2 = 2$ distinct sMoCA-based algorithms for probable PDD.

Finally, the Level II algorithms were based on the following sets of items:

$Attention = \{z(\textit{TMT-A}) < -1.5\ \cup z(WAIS\ DSB) < -1.5\}$

$Executive = \{z(CF\ A) < -1.5\ \cup z(\textit{PST-C}) < -1.5\}$

$Construction = \{z(JoL) < -1.5\ \cup z(\textit{CLOX-I}) < -1.5\}$

$Memory = \{z(\textit{RAVLT-DR}) < -1.5\ \cup z(\textit{BVMTR- DR}) < -1.5\ \cup z(WMS\text-III\ Family\ Pictures) < -1.5\}$

$Language = \{z(WAIS\ Similarities) < -1.5 \ \cup z(\textit{BNT-60}) < -1.5\}$

$IADL = \{FAQ > 7, {FAQ\ (it. 9)} > 1\}$

where $z()$ denotes calculation of age, sex and education adjusted z-score. This yields $1 \times 1 \times 1 \times 1 \times 1 \times 2 = 2$ distinct Level II algorithms for probable PDD in the current study. All but the BNT 60 item were evaluated using regression norms published by @bezdicek2017. Since the original article used BNT 30 instead of BNT 60, we approximated the deficit in BNT 60 by comparing patients' raw score to age- and education-specific normative values reported by @zemanova2016. Specifically, patients whose BNT 60 score fell below 5^th^ percentile of their demographic group in Table 6 of @zemanova2016 were considered to show signs of impaired performance.

### Operationalization of Impaired Cognition

In the original criteria, item 4 of Level I criteria, i.e., impaired cognition, was defined as follows: *"The proposed diagnostic criteria require a profile of cognitive deficits, typical of those described for PD-D, in two or more of four domains."* [@dubois2007, p. 2316] Consequently, we defined impaired cognition as a deficit in two or more domains of four in MMSE-based criteria and as a deficit in two or more of five domains in MoCA-based criteria. sMoCA-based criteria omitted the "impaired cognition" item altogether because they were intended as a shorter screening alternative to classical Level I assessment. Finally, for the Level II criteria, we employed standard definition of impaired cognition as the *"\[i\]mpairment on at least two neuropsychological tests, represented by either two impaired tests in one cognitive domain or one impaired test in two different cognitive domains."* [@litvan2012, Table 1]

```{r}
#| label: "tbl-algos"
#| tbl-cap: "Summary of all algortihms for probable PDD used in the study."

rates$gtables$algorithms
```

## Theoretical and Empirical Estimands

In this study, we follow the framework proposed by @lundberg2021 for specifying targets of inference (i.e., the estimands) in quantitative sciences to increase transparency and connect statistical evidence to relevant theory. @tbl-estimands contains verbal description of the components relating to each of our proclaimed research objectives and map them to the population quantity of interest (the theoretical estimand), data-dependent quantity that could be estimated (the empirical estimand) and quantities that are reported in the study (statistical estimates).

```{r}
#| label: "tbl-estimands"
#| tbl-cap: "Mapping between research objectives and quantities of interest in the current study."

table_estimands()
```

The RO1 - to estimate the PDD rate and evaluate the diagnostic variability and concordance across different algorithms of probable PDD - was divided into four distinct research objectives:

-   to estimate the rate of PDD within PD (RO1.1),
-   to estimated variability of this rate (RO1.2),
-   to evaluate predictive value of demographic variables for probable PDD classification (RO1.3) and
-   to evaluate concordance between different probable PDD operationalizations and criteria (RO1.4).

Estimates relating to RO1.1 and RO1.3 cannot be safely generalized beyond a population of PD patients that are candidates for DBS due to the systematic differences between DBS candidates pool and general PD population (such as the lower age of DBS candidates compared to the general PD population). On the other hand, the estimates relating to RO1.4 (and to a lesser degree to RO1.2[^3]) may not be substantially influenced by the sample at hand as the primary source of their variance might come from variability in measures employed (e.g., MMSE vs MoCA to assess global cognitive performance) rather than variability in patients' performance. Assuming that there is no substantial Differential Item Functioning for DBS candidates compared to a broader population of patients with PD, the estimates relating to RO1.4 can be cautiously generalized beyond the current sample.

[^3]: Because the quantity of interest is a rate and could thus be though of as a sum of binomially distributed PDD occurences divided by the total number of patients, its variance will likely systematically vary with its mean. Specifically, as the rate goes from extremes to 0.5, the variance increases. Consequently, if our estimate of the rate was lower than the true population rate, e.g., because our sample includes younger patients compared to the general PD population, our estimate of variance would also be lower than the true variance of PDD rate in the general PD population. Nonetheless, the between-algorithm variability may not be affected by this phenomenon as unlike variability of PDD rate, we do not have reason to assume it comes about by summing independent binomial events.

Finally, for the RO2, the theoretical estimand is defined as the set of diagnostic components whose variation systematically alters the probability of a probable PDD diagnosis. This aspect of the study is exploratory in nature. Empirically, we assess the contribution of each diagnostic feature by examining how variations in operational definitions (e.g., domain-specific thresholds, criteria for functional impairment) influence the statistical estimates derived for the first objective. This allows us to identify the diagnostic elements most responsible for between-algorithm discrepancies.

## Supplementary Presentation of Results

```{r}
#| label: "fig-rates"
#| fig-cap: "Summary of the estimates of probable PDD rate."
#| apa-note: "Vertical lines represent estimates arrived at by using sMoCA (dotted) or Level II (dashed) with FAQ item 9 (orange) or FAQ total score (blue) as criteria for probable PDD. The percentages were calculated from all available cases (see Table A3 for numerical summary)."

rates$plot
```

```{r}
#| label: "tbl-rates"
#| tbl-cap: "Estimates of the rate of probable PDD in the sample."

rates$gtables$rates
```

```{r}
#| label: "tbl-approx-bal"
#| tbl-cap: "Level I algorithms for probable PDD as predictors of Level II classification as the reference arranged by their balanced accuracy score."

approx_bal$gtable
```

```{r}
#| label: "fig-data"
#| fig-cap: "Representation of study data."
#| apa-note: "The figure shows whether patients (x-axis) ordered from the youngest (left) to the oldest (right) were classified as probable PDD by each tested algorithm (y-axis) ordered from the one with the lowest (bottom) to the highest (top) PDD rate estimate. Patients printed in red are women, patients printed in blue are men. Red cells indicate probable PDD diagnosis, grey cells indicate non-PDD diagnosis and white cells indicate missing diagnosis."

pred$plots$data
```

```{r}
#| label: "fig-pars"
#| fig-cap: "Summary of logistic regressions parameters prediction probable PDD by age and sex."
#| apa-note: "Histograms represent odd ratio (OR) estimates and p-values associated with age, sex, and their interaction as predictors of each of the 68 probable PDD classification. In the case of parameters for sex, values higher than 20 were omitted for clarity. Vertical lines indicate OR = 1 and p = .05."

pred$plots$parameters
```

```{r}
#| label: "fig-kappa"
#| fig-cap: "Cohen's κ matrix."
#| apa-note: "The matrix depicts Cohen's κ measuring agreement between algorithms for PDD. Algorithms printed in red defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response."

conc$plots$Kappa
```

```{r}
#| label: "fig-sens"
#| fig-cap: "Sensitivity matrix."
#| apa-note: "The matrix depicts sensitivity of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response."

conc$plots$Sensitivity
```

```{r}
#| label: "fig-spec"
#| fig-cap: "Specificity matrix."
#| apa-note: "The matrix depicts specificity of algorithms for PDD depicted on x-axis in predicting outcomes based on algorithms on the y-axis. Algorithms printed in blue defined IADL deficit by FAQ total score, algorithms printed in black defined IADL deficit by FAQ item 9 response."

conc$plots$Specificity
```
